---
title: "TP Clustering"
subtitle: "Partie 3 : Classification hiérarchique"
date : "4modIA / 2023-2024"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth : 4
    number_sections : true
header-includes:
  - \usepackage{comment}
params:
  soln: TRUE   
---

```{css,echo=F}
.badCode {
background-color: #cfdefc; 
}

.corrO { background-color: rgb(255,238,237); }
.corrS { background-color: pink; color: black; border: 1px solid red; }
```

```{r setup, echo=FALSE, cache=TRUE, message=FALSE,warning=FALSE}
library(knitr)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="badCode")
opts_knit$set(width=75)
```

L'objectif de ce TP est d'illustrer les notions abordées pour  classification hiérarchique. 
Les librairies R nécessaires pour ce TP : 

```{r,echo=T, error=F,warning=F,message=F}
library(mclust)
library(clusterSim)
library(factoextra)
library(FactoMineR)
library(ggplot2)
library(reshape2)
library(circlize)
library(viridis)
```

# Clustering des données de vin par CAH

On reprend dans ce TP les données `wine` disponibles sur la page moodle du cours. On charge ici les données.  

```{r,eval=F}
wine<-read.table("wine.txt",header=T)
wine$Qualite = as.factor(wine$Qualite)
wine$Type = factor(wine$Type, labels = c("blanc", "rouge"))
wineinit<-wine
wine[,-c(1,2)]<-scale(wine[,-c(1,2)],center=T,scale=T)
head(wine)
```

On fait une ACP pour la visualisation des résultats dans la suite

```{r,eval=F}
resacp<-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)
fviz_pca_ind(resacp,geom=c("point"),habillage=2)
```



**Question **: A l'aide de la fonction `hclust`, faites une classification hiérarchique des données de vins avec les mesures d'agrégation `single`, `complete` et `average` respectivement. Comparez visuellement les dendrogrammes associés. Commentez. 

```{r,eval=F}
# A COMPLETER
d = dist(wine[,-c(1,2)], "euclidian")
hclustsingle<-hclust(d, method = "single")
hclustcomplete<-hclust(d, method = "complete")
hclustaverage<-hclust(d, method = "average")

# Dendrogramme
plot(hclustsingle,hang=-1,labels=FALSE)
fviz_dend(hclustsingle,show_labels=FALSE)

plot(hclustcomplete,hang=-1,labels=FALSE)
fviz_dend(hclustcomplete,show_labels=FALSE)

plot(hclustaverage,hang=-1,labels=FALSE)
fviz_dend(hclustaverage,show_labels=FALSE)
```

esprit de chainage sur le plut haut de la hiérarchie
autres : 
complete ; au moment ou on coupe le dendo, effectifs par classe plus équilibrée contrairement à la simple

**Question :** Déduisez du dendrogramme avec la mesure d'agrégation `complete` une classification en 3 classes. Vous pouvez utiliser la fonction `cutree()`. Comparez-la avec les variables *Qualité* et *Type*. Commentez.  

```{r,eval=F}
ClassK3 = cutree(hclustcomplete,k=3)
table(cutree(hclustcomplete,k=3))
table(cutree(hclustcomplete,k=3), wine$Qualite)
table(cutree(hclustcomplete,k=3), wine$Type)
```




**Question : ** Tracez la distribution des variables quantitatives de `wine` en fonction de la classification en 3 classes de la question précédente. Commentez. 

```{r,eval=F}
df<-data.frame(wine[,-c(1,2)],Class=as.factor(ClassK3))
df<-melt(df,id="Class")
ggplot(df,aes(x=variable,y=value))+geom_violin(aes(fill=Class))
```
classes verte bleu : effet plus bleu sur so2 qui a l'air de jouer




**Question :** Dans cette question et pour les suivantes, on se focalise sur la mesure d'agrégation de Ward. Ajustez une classification hiérarchique avec la mesure de Ward. Que représentent les hauteurs du dendrogramme dans ce cas ? 

```{r,eval=F}
hward<-hclust(d, method = "ward.D2")
fviz_dend(hward,show_labels=FALSE)
hward$merge
```
agrege tel que inertie intra augmente le moins possible
à chaque valeur on lui donne une nouvelle classification avec un certain nombre de classes


**Question : ** Déterminez le nombre de classes à retenir avec l'indice de Calinski-Harabasz. Vous pouvez vous aider de la fonction `ìndex.G1()` de la librairie `clusterSim`. Tracez la classification obtenue sur le dendrogramme et sur le premier plan factoriel de l'ACP. 

```{r,eval=F}
# A completer
CH<-NULL
Kmax<-20
for (k in 2:Kmax){
  CH<-c(CH,index.G1(wine[,-c(1,2)], cutree(hward, k)))
}
daux<-data.frame(NbClust=2:Kmax,CH=CH)
ggplot(daux,aes(x=NbClust,y=CH))+geom_line()+geom_point()

ClustCH<-cutree(hward,which.max(CH)+1)
fviz_dend(hward, show_labels=FALSE, k=which.max(CH)+1)
fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(ClustCH))
fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(ClustCH), axes = c(1,3))

fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(cutree(hward, k=4)), axes = c(1,3))
```
4ème classe s'est formée plus tard dans une autre direction
c proportionnel à l'inertie intra classe

**Question : ** Déterminez le nombre de classes à retenir avec le critère Silhouette. Vous pouvez vous aider de la fonction `ìndex.S()`  de la librairie `clusterSim`. Comparez avec la classification de la question précédente. 

```{r,eval=F}
# A COMPLETER
CH<-NULL
Kmax<-20
for (k in 2:Kmax){
  CH<-c(CH,index.S(d, cutree(hward, k)))
}
daux<-data.frame(NbClust=2:Kmax,CH=CH)
ggplot(daux,aes(x=NbClust,y=CH))+geom_line()+geom_point()

ClustCH<-cutree(hward,which.max(CH)+1)
fviz_dend(hward, show_labels=FALSE, k=which.max(CH)+1)
fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(ClustCH))
fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(ClustCH), axes = c(1,3))

fviz_pca_ind(resacp, geom = c("point"), habillage = as.factor(cutree(hward, k=4)), axes = c(1,3))
```


```{r,eval=F}
CH<-NULL
Kmax<-20
for (k in 2:Kmax){
  CH<-c(CH,index.S(d, cutree(hward, k)))
}
daux1<-data.frame(NbClust=1:Kmax,Intra=rev(hward$height[tail(1:nrow(wine), Kmax)-1]))
ggplot(daux1,aes(x=NbClust,y=Intra))+geom_line()+geom_point()
```


**Question :** Comparez la classification obtenue avec la méthode des Kmeans dans le TP 1 et celle obtenue à la question précédente. 

```{r}
clus1F = paste("CKM", reskmeans$cluster, sep="")
clus1F = paste("CAH", cutree(hward, 4), sep="")

mycolor = viridis(0, alpha = 1, begin = 0, end = 1, option = "H")
mycolor = mycolor[sample(1:8)]
chordDiagram(table(clus1F, clus2F), grid.col = mycolor)
```
# Clustering des données Zoo par CAH

## Lecture des données et fonctions auxiliaires

On reprend dans cette partie le jeu de données zoo ainsi que les fonctions auxiliaires comme dans le TP1. On refait une analyse en composantes multiples pour la suite. 

```{r,eval=F}
zoo<-read.table("zoo-dataTP.txt",header=T,stringsAsFactors = TRUE)
for (j in 1:ncol(zoo))
  zoo[,j]<-as.factor(zoo[,j])
summary(zoo)
library("FactoMineR")
library("factoextra")
res.mca<- MCA(zoo,ncp = 5, graph = FALSE)

fviz_screeplot(res.mca)
plot(res.mca, invisible = c("quali.sup", "ind"), cex=1, col.var = "darkblue", cex.main=2, col.main= "darkblue")

fviz_mca_ind(res.mca)
```

Pour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction `barplotClus()` permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d'un clustering donné. 

```{r,eval=F}
# J indice des variables
# Data = jeu de données
# clust = clustering étudié
# output : graphe par variable dans J donnant la répartition des modalités de J par classe de clust

barplotClus <- function(clust, Data, J) {
    aux.long.p <- heatm(clust, Data, J)$freq
    # ux<-unique(aux.long.p$variable)
    for (j in J) {
        p <- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[j]), ],
            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = "identity")+
            labs(fill = colnames(Data)[j])
        print(p)
    }
}

heatm <- function(clust, Data, J) {
    library(dplyr)
    Dataaux <- data.frame(id.s = c(1:nrow(Data)), Data)
    aux <- cbind(Dataaux, clust)
    aux.long <- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,
        id = c("id.s", "clust"), factorsAsStrings = T)
    # Effectifs
    aux.long.q <- aux.long %>%
        group_by(clust, variable, value) %>%
        mutate(count = n_distinct(id.s)) %>%
        distinct(clust, variable, value, count)
    # avec fréquences
    aux.long.p <- aux.long.q %>%
        group_by(clust, variable) %>%
        mutate(perc = count/sum(count)) %>%
        arrange(clust)

    Lev <- NULL
    for (j in 1:ncol(Data)) Lev <- c(Lev, levels(Data[, j]))

    Jaux <- NULL
    for (j in 1:length(J)) {
        Jaux <- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))
    }

    gaux <- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +
        scale_fill_gradient2(low = "white", mid = "blue", high = "red") + theme_minimal()

    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))
}

```

## CAH directe sur variables qualitatives


**Question :** Quelle classification par CAH pouvez-vous mettre en place pour traiter des données qualitatives ? Mettez en application votre proposition avec R et étudiez la classification retenue. Vous pourrez vous aider des fonctions `daisy()` de la librairie `cluster`, de la fonction `hclust()`, ...

```{r,eval=F}
dzoo = daisy(zoo, metric="gower")
hclustzooS = hclust(dzoo, method = "single")
plot(hclustzooS,hang=-1,labels=FALSE)
fviz_dend(hclustzooS,show_labels=FALSE)

hclustzooC = hclust(dzoo, method = "complete")
plot(hclustzooC,hang=-1,labels=FALSE)
fviz_dend(hclustzooC,show_labels=FALSE, k=9)

hclustzooA = hclust(dzoo, method = "average")
plot(hclustzooA,hang=-1,labels=FALSE)
fviz_dend(hclustzooA,show_labels=FALSE)

```
on part sur de la complete

```{r}
aggl = hclustzooC
ggplot(data.frame(K=1:20, Height=sort(aggl$height, decreasing = T)[1:20]), aes(x=K,y=Height)) + geom_line()+geom_point()
```
```{r}
clusaggl = cutree(aggl, 9)
fviz_pca_ind(res.mca, geom.ind = c("point","text"), habillage = as.factor(clusaggl))
```


```{r}
table(clusaggl)
```

## CAH sur les coordonnées de MCA

**Question :** Mettez en place une classification des animaux à partir des coordonnées de l'analyse en composantes multiples. Comparez avec la classification obtenue dans la section précédente. 


```{r,eval=F}
d = dist(res.mca$ind$coord, method = "euclidian")
hward = hclust(d, method = "ward.D2")
clusintra = cutree(hward, k=4)
table(clusintra, clusaggl)
```
```{r}
adjustedRandIndex(clusintra, clusaggl)
```

classe 2 pas ouf car petits bouts par ci par là
ARI assez haut mais comme c globalement si on se mettait à aggreger on tomberzait sur les memes classes
il y a une certaine cohérence 

important : - choix des distances et mesures d'aggregation
avantage : pas de random donc on retrouve tous la meme classification




